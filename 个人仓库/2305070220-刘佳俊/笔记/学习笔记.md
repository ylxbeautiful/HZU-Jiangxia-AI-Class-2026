**一．数据预处理**



数据预处理流程



1\. 创建原始数据：手动构建CSV文件

2\. 加载数据：用pandas读取

3\. 拆分特征和标签：分离输入和输出

4\. 处理缺失值

5.转为张量：为深度学习模型准备





fillna()	填充缺失值

get\_dummies()  将分类变量转换为虚拟/指标变量

dummy\_na参数控制是否处理缺失值

astype('float32')	类型转换	PyTorch默认float32















**二．创建张量及其基础操作**

1.torch.arange() ：创建张量（默认从0开始，步长为1）

x.shape：查看张量形状

x.numel()：查看元素总数

reshape() 重塑形状，不改变元素总数，只改变排列方式

**思考：reshape时元素总数必须匹配吗？如果不匹配，多出来和少的会影响整个张量，所以要匹配**

2.torch.zeros()	创建全0张量

torch.ones()	创建全1张量

torch.tensor()   创建指定数值

torch.full((指定形状)，填充数值)

**思考：有时候想创建例如全为x张量，可以使用全1张量乘x或者全0张量加x**





3.创建新张量：虽然最后内存与原地操作的一样，但是过程中内存峰值高（适用需要保留原数据的场景）

原地操作：会直接修改原数据，高效率，带下划线的版本都是原地操作，如x.add\_(5)  ，x.pow\_(2)

思考：原地操作会直接修改数据，那自动求导是不是不能使用原地操作



4\.

id()： 检查内存地址

clone()创建完整的数据副本，新张量有独立的内存地址

cumsum()：累积求和



torch.cat() 用于在指定维度上连接多个张量（连接的张量在非连接维度上必须形状相同）

当张量为3维时：

dim = 0 ：块数合并，增加块数

dim = 1 : 行数合并，增加行数

dim = 2: 列数合并，增加列数

















**三．计算：**

1.加法: x + y

减法: x - y

乘法: x \* y        #并非矩阵乘法，是逐元素乘法（对应位置的元素相乘）

除法: x / y

幂运算:  x \*\* y

指数运算:torch.exp(x)

（PyTorch中的基本算术运算都是逐元素执行的，即对应位置的元素进行计算）





2.torch.dot(x, y)  # 向量点积

torch.sum(x \* y)  # 等价于点积

torch.mv(X, y)  # 矩阵与向量相乘

torch.mm(X, Y)  # 矩阵相乘，仅适用于2维





3.广播机制：允许形状不同的张量进行运算，自动将较小的张量扩展到大张量的形状

常见类型：(n,1) 和 (1,m) → (n,m)





4\.

X.sum() 默认将所有元素相加，返回标量，也可指定维度进行部分求和

**思考：(X == Y).sum()  可以统计相等元素个数**









不指定axis时，对整个张量进行聚合，返回标量（0维张量）



指定轴求和（维度约减）时：

A.sum(axis=0)  # 沿第0维合并，移除第0维

A.sum(axis=1)  # 沿第1维合并 ，移除第1维

A.sum(axis=2)  # 沿第2维合并，移除第2维

也可以多个轴同时聚合，同时沿多个维度求和，就去掉更多的维度



**思考:求和就是沿着某个方向压扁张量，移除某个方向的维度**







5.保持维度的聚合



 keepdim=True 保持维度数量，使之为一



**思考: 保留了维度结构，应该是方便后续广播操作**











6.广播除法（归一化）



A / A.sum(axis=1, keepdims=True)  # 按行归一化

A / A.sum(axis=2, keepdims=True)  # 按列归一化

就是每个元素对这部分的权重











**四.自动求导**



1.x.requires\_grad\_(True)  # 启用梯度跟踪

2..detach()	切断计算图追踪，返回一个新的张量，与原始张量共享数据但不参与梯度计算

3..grad  	存储计算好的梯度	调用backward()前为None

4.with torch.no\_grad(): 上下文管理器，临时禁用梯度跟踪，在该上下文中的所有计算都不会构建计算图

5.backward() 的 retain\_graph=True 参数，在执行反向传播后保留计算图，允许多次 backward 调用


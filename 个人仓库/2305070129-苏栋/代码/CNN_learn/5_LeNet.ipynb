{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet是第一个卷积神经网络，由Yann LeCun在1989年提出，并在1998年发表。LeNet在图像分类任务上取得了很好的效果，是深度学习领域的奠基性工作之一。\n",
    "\n",
    "分为两个部分：卷积层和全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5, strides=1, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5, strides=1, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense层会默认将(批量大小, 通道, 高, 宽)形状的输入转换成(批量大小, 通道*高*宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 16, 8, 8)\n",
      "pool1 output shape:\t (1, 16, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X=nd.random.uniform(shape=(1,1,28,28)) # 长和宽均为28的单通道样本\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu():\n",
    "    try:\n",
    "        ctx=mx.gpu()\n",
    "        _=nd.zeros((1,),ctx=ctx)\n",
    "    except:\n",
    "        ctx=mx.cpu()\n",
    "    return ctx\n",
    "ctx=try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net, ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=(net(X).argmax(axis=1)==y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n\n",
    "def train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()\n",
    "            train_acc_sum+=(y_hat.argmax(axis=1)==y).sum().asscalar()\n",
    "            n+=y.size\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.3168, train acc 0.108, test acc 0.189, time 27.1 sec\n",
      "epoch 2, loss 1.3413, train acc 0.470, test acc 0.627, time 27.1 sec\n",
      "epoch 3, loss 0.8670, train acc 0.659, test acc 0.708, time 24.9 sec\n",
      "epoch 4, loss 0.7295, train acc 0.715, test acc 0.739, time 26.0 sec\n",
      "epoch 5, loss 0.6486, train acc 0.744, test acc 0.762, time 25.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.9,5\n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2 output shape:\t (1, 6, 26, 26)\n",
      "pool2 output shape:\t (1, 6, 13, 13)\n",
      "conv3 output shape:\t (1, 16, 9, 9)\n",
      "pool3 output shape:\t (1, 16, 4, 4)\n",
      "dense3 output shape:\t (1, 120)\n",
      "dense4 output shape:\t (1, 84)\n",
      "dense5 output shape:\t (1, 10)\n",
      "training on cpu(0)\n",
      "epoch 1, loss 2.3189, train acc 0.104, test acc 0.100, time 16.3 sec\n",
      "epoch 2, loss 1.5385, train acc 0.412, test acc 0.652, time 22.7 sec\n",
      "epoch 3, loss 0.8479, train acc 0.670, test acc 0.718, time 22.6 sec\n",
      "epoch 4, loss 0.7208, train acc 0.717, test acc 0.742, time 23.8 sec\n",
      "epoch 5, loss 0.6494, train acc 0.744, test acc 0.761, time 22.0 sec\n"
     ]
    }
   ],
   "source": [
    "# 调整卷积窗口大小\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=3, strides=1, activation='sigmoid'), # nn.Conv2D参数：6表示输出通道数，5表示卷积核大小，1表示步幅，sigmoid表示激活函数\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5, strides=1, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense层会默认将(批量大小, 通道, 高, 宽)形状的输入转换成(批量大小, 通道*高*宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))\n",
    "\n",
    "X=nd.random.uniform(shape=(1,1,28,28)) # 长和宽均为28的单通道样本\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)\n",
    "\n",
    "def evaluate_accuracy(data_iter,net, ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=(net(X).argmax(axis=1)==y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n\n",
    "def train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()\n",
    "            train_acc_sum+=(y_hat.argmax(axis=1)==y).sum().asscalar()\n",
    "            n+=y.size\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "lr,num_epochs=0.9,5\n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积窗口大小决定了卷积层提取特征的感受野（Receptive Field）。\n",
    "\n",
    "影响：\n",
    "\n",
    "较小的卷积窗口（如 3x3）：\n",
    "\n",
    "提取更细粒度的局部特征。\n",
    "\n",
    "计算量较小，适合深层网络。\n",
    "\n",
    "在图像分类任务中，通常使用多个小卷积核堆叠（如 VGG 网络）来替代大卷积核，以减少参数数量并增加非线性。\n",
    "\n",
    "较大的卷积窗口（如 5x5 或 7x7）：\n",
    "\n",
    "提取更大范围的全局特征。\n",
    "\n",
    "计算量较大，可能导致过拟合。\n",
    "\n",
    "适合输入图像较大且需要捕捉全局信息的任务。\n",
    "\n",
    "实验结果：\n",
    "\n",
    "使用较小的卷积窗口（如 3x3）通常可以获得更好的性能，尤其是在深层网络中。\n",
    "\n",
    "较大的卷积窗口可能会导致模型过拟合，尤其是在数据量较少的情况下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4 output shape:\t (1, 10, 24, 24)\n",
      "pool4 output shape:\t (1, 10, 12, 12)\n",
      "conv5 output shape:\t (1, 16, 8, 8)\n",
      "pool5 output shape:\t (1, 16, 4, 4)\n",
      "dense6 output shape:\t (1, 120)\n",
      "dense7 output shape:\t (1, 84)\n",
      "dense8 output shape:\t (1, 10)\n",
      "training on cpu(0)\n",
      "epoch 1, loss 2.3153, train acc 0.105, test acc 0.165, time 25.0 sec\n",
      "epoch 2, loss 1.3913, train acc 0.458, test acc 0.615, time 43.3 sec\n",
      "epoch 3, loss 0.8437, train acc 0.670, test acc 0.730, time 47.1 sec\n",
      "epoch 4, loss 0.6965, train acc 0.726, test acc 0.745, time 45.8 sec\n",
      "epoch 5, loss 0.6172, train acc 0.757, test acc 0.771, time 33.6 sec\n"
     ]
    }
   ],
   "source": [
    "# 调整输出通道数\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(10, kernel_size=5, strides=1, activation='sigmoid'), # nn.Conv2D参数：6表示输出通道数，5表示卷积核大小，1表示步幅，sigmoid表示激活函数\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5, strides=1, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense层会默认将(批量大小, 通道, 高, 宽)形状的输入转换成(批量大小, 通道*高*宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))\n",
    "\n",
    "X=nd.random.uniform(shape=(1,1,28,28)) # 长和宽均为28的单通道样本\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)\n",
    "\n",
    "def evaluate_accuracy(data_iter,net, ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=(net(X).argmax(axis=1)==y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n\n",
    "def train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()\n",
    "            train_acc_sum+=(y_hat.argmax(axis=1)==y).sum().asscalar()\n",
    "            n+=y.size\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "lr,num_epochs=0.9,5\n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出通道数决定了卷积层提取的特征图数量。\n",
    "\n",
    "影响：\n",
    "\n",
    "较多的输出通道：\n",
    "\n",
    "提取更丰富的特征，增强模型的表达能力。\n",
    "\n",
    "计算量和参数数量增加，可能导致过拟合。\n",
    "\n",
    "需要更多的训练数据来充分训练模型。\n",
    "\n",
    "较少的输出通道：\n",
    "\n",
    "计算量较小，训练速度更快。\n",
    "\n",
    "可能无法充分捕捉数据的复杂特征，导致欠拟合。\n",
    "\n",
    "实验结果：\n",
    "\n",
    "增加输出通道数通常会提高模型的性能，但也会增加计算复杂度。\n",
    "\n",
    "如果通道数过多，可能会导致过拟合，尤其是在数据量较少的情况下。\n",
    "\n",
    "通常需要在模型复杂度和性能之间找到平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv8 output shape:\t (1, 6, 24, 24)\n",
      "pool8 output shape:\t (1, 6, 12, 12)\n",
      "conv9 output shape:\t (1, 16, 8, 8)\n",
      "pool9 output shape:\t (1, 16, 4, 4)\n",
      "dense12 output shape:\t (1, 120)\n",
      "dense13 output shape:\t (1, 84)\n",
      "dense14 output shape:\t (1, 10)\n",
      "training on cpu(0)\n",
      "epoch 1, loss 2.3157, train acc 0.105, test acc 0.100, time 18.2 sec\n",
      "epoch 2, loss 2.3036, train acc 0.100, test acc 0.100, time 26.6 sec\n",
      "epoch 3, loss 2.3035, train acc 0.101, test acc 0.100, time 26.4 sec\n",
      "epoch 4, loss 2.3036, train acc 0.097, test acc 0.100, time 26.2 sec\n",
      "epoch 5, loss 2.3036, train acc 0.097, test acc 0.100, time 23.4 sec\n"
     ]
    }
   ],
   "source": [
    "# 调整激活函数\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5, strides=1, activation='relu'), # nn.Conv2D参数：6表示输出通道数，5表示卷积核大小，1表示步幅，sigmoid表示激活函数\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5, strides=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense层会默认将(批量大小, 通道, 高, 宽)形状的输入转换成(批量大小, 通道*高*宽)形状的输入\n",
    "        nn.Dense(120, activation='relu'),\n",
    "        nn.Dense(84, activation='relu'),\n",
    "        nn.Dense(10))\n",
    "\n",
    "X=nd.random.uniform(shape=(1,1,28,28)) # 长和宽均为28的单通道样本\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)\n",
    "\n",
    "def evaluate_accuracy(data_iter,net, ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=(net(X).argmax(axis=1)==y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n\n",
    "def train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()\n",
    "            train_acc_sum+=(y_hat.argmax(axis=1)==y).sum().asscalar()\n",
    "            n+=y.size\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "lr,num_epochs=0.9,5\n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结果可以看出，将激活函数从 Sigmoid 改为 ReLU 后，模型的性能显著下降，训练准确率和测试准确率都停留在 10% 左右（对于 10 类分类任务来说，这相当于随机猜测）。这表明模型在训练过程中出现了问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv10 output shape:\t (1, 6, 24, 24)\n",
      "pool10 output shape:\t (1, 6, 12, 12)\n",
      "conv11 output shape:\t (1, 16, 8, 8)\n",
      "pool11 output shape:\t (1, 16, 4, 4)\n",
      "dense15 output shape:\t (1, 120)\n",
      "dense16 output shape:\t (1, 84)\n",
      "dense17 output shape:\t (1, 10)\n",
      "training on cpu(0)\n",
      "epoch 1, loss 1.6687, train acc 0.412, test acc 0.633, time 24.8 sec\n",
      "epoch 2, loss 0.9429, train acc 0.650, test acc 0.695, time 29.1 sec\n",
      "epoch 3, loss 0.8081, train acc 0.696, test acc 0.702, time 28.3 sec\n",
      "epoch 4, loss 0.7349, train acc 0.721, test acc 0.749, time 23.8 sec\n",
      "epoch 5, loss 0.6913, train acc 0.737, test acc 0.761, time 23.3 sec\n"
     ]
    }
   ],
   "source": [
    "# 调整激活函数与学习率\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5, strides=1, activation='relu'), # nn.Conv2D参数：6表示输出通道数，5表示卷积核大小，1表示步幅，sigmoid表示激活函数\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(16, kernel_size=5, strides=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense层会默认将(批量大小, 通道, 高, 宽)形状的输入转换成(批量大小, 通道*高*宽)形状的输入\n",
    "        nn.Dense(120, activation='relu'),\n",
    "        nn.Dense(84, activation='relu'),\n",
    "        nn.Dense(10))\n",
    "\n",
    "X=nd.random.uniform(shape=(1,1,28,28)) # 长和宽均为28的单通道样本\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.name,'output shape:\\t',X.shape)\n",
    "\n",
    "def evaluate_accuracy(data_iter,net, ctx):\n",
    "    acc_sum,n=nd.array([0],ctx=ctx),0\n",
    "    for X,y in data_iter:\n",
    "        X,y=X.as_in_context(ctx),y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum+=(net(X).argmax(axis=1)==y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar()/n\n",
    "def train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X,y=X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y=y.astype('float32')\n",
    "            train_l_sum+=l.asscalar()\n",
    "            train_acc_sum+=(y_hat.argmax(axis=1)==y).sum().asscalar()\n",
    "            n+=y.size\n",
    "        test_acc=evaluate_accuracy(test_iter,net,ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "lr,num_epochs=0.01,5\n",
    "net.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试降低学习率，避免梯度爆炸或权重更新不稳定。将学习率从 0.9 调整为 0.01 ，调整学习率到 0.01 后，模型的训练过程和性能都趋于稳定，且没有出现过拟合现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

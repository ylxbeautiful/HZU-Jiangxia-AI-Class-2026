{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量归一化层\n",
    "\n",
    "对全连接层作批量归一化，可以按以下步骤实现：\n",
    "\n",
    "1. 按特征维将输入数据拆分为大小为 $n$ 的数据块，其中 $n$ 是批量大小。\n",
    "2. 对每个数据块，计算其均值和方差，再做批量归一化。\n",
    "3. 将数据块重新组合。\n",
    "\n",
    "对卷积层作批量归一化，可以按以下步骤实现：\n",
    "\n",
    "1. 对输入的通道维拆分为大小为 $n$ 的数据块，其中 $n$ 是通道数。\n",
    "2. 对每个数据块，计算其均值和方差，再做批量归一化。\n",
    "3. 将数据块重新组合。\n",
    "\n",
    "预测时的批量归一化\n",
    "\n",
    "* 在训练时，批量归一化层在每次迭代中计算当前小批量数据的均值和方差，并使用这些均值和方差对输入数据做批量归一化。在预测时，我们通常使用训练时最后一个小批量数据的均值和方差，或者通过移动平均对这些均值和方差进行估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, contrib, gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps,momentum):\n",
    "    # 通过autograd标记需要计算梯度\n",
    "    if not autograd.is_training():\n",
    "        # 如果是在预测阶段，直接使用移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        # 如果是在训练阶段，使用当前的均值和方差\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 对于全连接层，计算特征维上的均值和方差\n",
    "            mean = X.mean(axis=0)\n",
    "            var = ((X - mean)**2).mean(axis=0)\n",
    "        else:\n",
    "            # 对于卷积层，计算通道维上的均值和方差\n",
    "            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "            var = ((X - mean)**2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "        # 训练阶段，用当前的均值和方差做批量归一化\n",
    "        X_hat = (X - mean) / nd.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = (1 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1 - momentum) * moving_var + momentum * var\n",
    "    # 缩放和平移\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self, num_features, num_dims, **kwargs):\n",
    "        super(BatchNorm, self).__init__(**kwargs)\n",
    "        # num_dims=2表示全连接层，num_dims=4表示卷积层\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # gamma初始化为1，beta初始化为0\n",
    "        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n",
    "        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 在训练模式下更新移动平均的均值和方差\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.as_in_context(X.context)\n",
    "            self.moving_var = self.moving_var.as_in_context(X.context)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用批量归一化层的LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential()\n",
    "net.add(nn.Conv2D(6, kernel_size=5),\n",
    "       BatchNorm(6, num_dims=4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "       nn.Conv2D(16, kernel_size=5),\n",
    "       BatchNorm(16, num_dims=4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "       nn.Flatten(),\n",
    "       nn.Dense(120),\n",
    "       BatchNorm(120, num_dims=2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(84),\n",
    "       BatchNorm(84, num_dims=2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 0.8537, train acc 0.681, test acc 0.808, time 57.2 sec\n",
      "epoch 2, loss 0.4769, train acc 0.828, test acc 0.842, time 53.3 sec\n",
      "epoch 3, loss 0.3950, train acc 0.857, test acc 0.871, time 50.4 sec\n",
      "epoch 4, loss 0.3598, train acc 0.871, test acc 0.880, time 48.5 sec\n",
      "epoch 5, loss 0.3388, train acc 0.877, test acc 0.880, time 48.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size=0.9,5,256\n",
    "ctx=d2l.try_gpu()\n",
    "net.initialize(init.Normal(sigma=0.01), ctx=ctx, force_reinit=True)\n",
    "trainer=gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
